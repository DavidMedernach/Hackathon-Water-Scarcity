{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Training the *Final* Models\n",
    "\n",
    "This notebook trains the model on the full *baseline_dataset* for the final prediction on evaluation data.\n",
    "\n",
    "Here, we train a model designed to generalize across water stations in Brazil and France. However, you are not required to follow this approach and may opt to train separate models for different geographic *regions*.\n",
    "\n",
    "This baseline model training example utilizes all available features, with hyperparameters chosen for quick execution rather than optimization. For hyperparameter tuning and feature selection explorations, refer to the `02_exploration` folder.\n",
    "\n",
    "> **Note:** This notebook requires outputs from the `00 Preprocessing` notebooks.\n",
    "\n",
    "<img src=\"../images/notebook-3.png\" alt=\"Experiment Diagram\" style=\"width:75%; text-align:center;\" />\n",
    "\n",
    "### 1. Data Import and Setup\n",
    "\n",
    "This section imports the necessary libraries, sets up environment paths, and includes custom utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from mapie.regression import MapieQuantileRegressor\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..')))\n",
    "\n",
    "from src.utils.model import split_dataset, compare_models_per_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constants :\n",
    "- **INPUT_DIR**: Directory for input data (same as in \"02 - Feature Engineering\").\n",
    "- **MODEL_DIR**: Directory where trained models are saved.\n",
    "- **DATASET_DIR**: Directory where the Zenodo dataset is unzipped.\n",
    "\n",
    "##### Model Parameters\n",
    "\n",
    "- **SEED**: 42 (for reproducibility)\n",
    "- **NUMBER_OF_WEEK**: 4 (one model is trained per week)\n",
    "\n",
    "##### FINAL_MODELS\n",
    "\n",
    "- **mapie**: Combines LightGBM with MAPIE. **MAPIE** (Model Agnostic Prediction Interval Estimator) computes prediction intervals for any regression model using conformal methods.\n",
    "- **qrf**: Quantile Random Forest (natively produces prediction intervals)\n",
    "- **ebm**: Explainable Boosting Machine is used as a exemple that does not natively implement prediction intervals, but that can be customised to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../../../data/input/\"\n",
    "MODEL_DIR = \"../../../models/\"\n",
    "DATASET_DIR = \"../../../dataset/\"\n",
    "\n",
    "SEED = 42\n",
    "NUMBER_OF_WEEK = 4 # Number of weeks to predict one model is trained per week\n",
    "\n",
    "FINAL_MODELS = [\"mapie\",\n",
    "                \"qrf\",\n",
    "                #\"EBM\"\n",
    "                ]\n",
    "mapie_enbpi = {}\n",
    "mapie = {}\n",
    "qrf = {}\n",
    "mapie_aci = {}\n",
    "\n",
    "COLUMNS_TO_DROP = [\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading\n",
    "Load in the baseline datasets, create the directory to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv(f\"{INPUT_DIR}dataset_baseline.csv\")\n",
    "\n",
    "dataset_train = dataset_train.set_index(\"ObsDate\")\n",
    "\n",
    "if not os.path.exists(f\"{MODEL_DIR}final/\"):\n",
    "    os.makedirs(f\"{MODEL_DIR}final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing removal of unnecessary columns, setup of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset_train.drop(columns=COLUMNS_TO_DROP)\n",
    "y_train = {}\n",
    "for i in range(0, NUMBER_OF_WEEK):\n",
    "    y_train[i] = dataset_train[f\"water_flow_week{i+1}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models training\n",
    "#### a. LGBM + MAPIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapie Model Training Overview\n",
    "\n",
    "- **Configuration:**  \n",
    "  - Sets `ALPHA` (0.1) as the prediction interval level.\n",
    "  - Defines `TIME_VALIDATION` as a split point for creating a validation set.\n",
    "  - Configures LightGBM parameters (`LGBM_PARAMS`) for quantile regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "TIME_VALIDATION = \"2000-01-01 00:00:00\"\n",
    "\n",
    "LGBM_PARAMS = {\n",
    "    \"max_depth\": 15,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"n_estimators\": 500,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"objective\": \"quantile\",\n",
    "    \"alpha\": ALPHA\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Preparation:**  \n",
    "  - Splits `dataset_train` into training and validation subsets using `split_dataset`.\n",
    "  - Removes unnecessary columns from both the training and validation datasets.\n",
    "  - Extracts target variables for each week (from `water_flow_week1` to `water_flow_week4`).\n",
    "\n",
    "- **Model Training:**  \n",
    "  For each week:\n",
    "  - Initializes a LightGBM regressor with the specified parameters.\n",
    "  - Wraps it in a `MapieQuantileRegressor` to estimate prediction intervals.\n",
    "  - Trains the model on the training data and calibrates it using the validation data.\n",
    "  - Saves the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mapie\n",
      "154\n",
      "Training week 0\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 0.218643\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 541.075745\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 10.800000\n",
      "Training week 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 0.218929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 542.302002\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 10.837143\n",
      "Training week 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 0.219071\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 543.414124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 10.885586\n",
      "Training week 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 0.219143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 544.039490\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 10.898714\n"
     ]
    }
   ],
   "source": [
    "if \"mapie\" in FINAL_MODELS: \n",
    "    print(\"Training Mapie\")\n",
    "\n",
    "\n",
    "    train_mapie, val_mapie, val_temporal  = split_dataset(dataset_train, 0.75, TIME_VALIDATION)\n",
    "\n",
    "    X_train_mapie = train_mapie.drop(columns=COLUMNS_TO_DROP)\n",
    "    print(len(X_train_mapie.columns))\n",
    "    y_train_mapie = {}\n",
    "    for i in range(0, NUMBER_OF_WEEK):\n",
    "        y_train_mapie[i] = train_mapie[f\"water_flow_week{i+1}\"]\n",
    "\n",
    "    X_val = val_mapie.drop(columns=COLUMNS_TO_DROP)\n",
    "    y_val = {}\n",
    "    y_val[0] = val_mapie[\"water_flow_week1\"]\n",
    "    for i in range(1, NUMBER_OF_WEEK):\n",
    "        y_val[i] = val_mapie[f\"water_flow_week{i+1}\"]\n",
    "\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training week {i}\")\n",
    "        # Initialize and train MapieQuantileRegressor\n",
    "        regressor = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "        mapie[i] = MapieQuantileRegressor(estimator=regressor, method=\"quantile\", cv=\"split\", alpha=ALPHA)\n",
    "        mapie[i].fit(X_train_mapie, y_train_mapie[i], X_calib=X_val, y_calib=y_val[i])\n",
    "        \n",
    "        # save model with date\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        model_path = f\"{MODEL_DIR}final/mapie_quantile_{time}_week_{i}.pkl\"\n",
    "        joblib.dump(mapie[i], model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. QRF\n",
    "\n",
    "- **Training:**  \n",
    "  Initializes a `RandomForestQuantileRegressor` with the following parameters:\n",
    "  - 100 estimators\n",
    "  - Maximum depth of 10\n",
    "  - Minimum of 10 samples per leaf\n",
    "\n",
    "  These parameters allow for relatively fast training, though they are not optimized for peak performance. \n",
    "  \n",
    "  The model is then fitted using `X_train` and the corresponding weekly target `y_train[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training week 0\n",
      "Training week 1\n",
      "Training week 2\n",
      "Training week 3\n"
     ]
    }
   ],
   "source": [
    "if \"qrf\" in FINAL_MODELS:\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training week {i}\")\n",
    "        # Train RandomForestQuantileRegressor\n",
    "        qrf[i] = RandomForestQuantileRegressor(n_estimators=100, max_depth=5, min_samples_leaf=5)\n",
    "        qrf[i].fit(X_train, y_train[i])\n",
    "\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        model_path = f\"{MODEL_DIR}final/qrf_quantile_{time}_week_{i}.pkl\"\n",
    "        joblib.dump(qrf[i], model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Explainable Boosting Machine\n",
    "\n",
    "EBM is an ensemble method that does not natively provide access to its individual members for performing quantile predictions or generating prediction intervals. To overcome this limitation, we manually construct an ensemble.\n",
    "\n",
    "\n",
    "- **Ensemble Training:**  \n",
    "- For each ensemble member (seed from 0 to 4):\n",
    "    - A bootstrap sample is created from `X_train` and `y_train[i]` using sampling with replacement.\n",
    "    - An `ExplainableBoostingRegressor` is instantiated with fixed parameters (e.g., `max_bins=128`, `learning_rate=0.05`, `interactions=3`, and `random_state=42` to ensure consistent binning) and then trained on the sampled data.\n",
    "    - The trained model is appended to the list for the current week.\n",
    "- **Saving the Ensemble:**  \n",
    "- The ensemble (i.e., the list of EBM models for the week) is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ebm\" in FINAL_MODELS:\n",
    "    NUM_ENSEMBLES = 5\n",
    "    ebm_ensembles = {}\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training EBM ensemble for week {i}\")\n",
    "\n",
    "        models_i = []\n",
    "        for seed in range(NUM_ENSEMBLES):\n",
    "            print(f\"Training EBM ensemble {seed} for week {i}\")\n",
    "            # 1. Create your bootstrap sample or subset (if you want bagging)\n",
    "            sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "            X_sample = X_train.iloc[sample_indices]\n",
    "            y_sample = y_train[i][sample_indices]\n",
    "            \n",
    "            # 2. Train an EBM with consistent binning parameters\n",
    "            ebm_model = ExplainableBoostingRegressor(\n",
    "                outer_bags=1,\n",
    "                inner_bags=1,\n",
    "                max_bins=128,\n",
    "                learning_rate=0.05,\n",
    "                interactions=3,\n",
    "                early_stopping_rounds=100,\n",
    "                random_state=SEED\n",
    "            )\n",
    "            ebm_model.fit(X_sample, y_sample)\n",
    "            \n",
    "            models_i.append(ebm_model)\n",
    "\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        file_path = f\"{MODEL_DIR}final/ebm_ensemble_{time}_week_{i}.pkl\"\n",
    "\n",
    "        joblib.dump(ebm_ensembles, file_path)\n",
    "        print(f\"Saved EBM ensembles to {file_path}\")\n",
    "\n",
    "        # Store the list of models for week i\n",
    "        ebm_ensembles[i] = models_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Bayesian Structural Time Series with Variational Inference method\n",
    "\n",
    "Note: This section is used for training and prediction for Bayesian method. For other method refer to the \"Prediction Computation\" notebook for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import zipfile\n",
    "\n",
    "from scipy.stats import iqr\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_DIR = \"../../../data/evaluation/\"\n",
    "COLUMNS_TO_DROP_STS = [\"water_flow_week1\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28431, 160) (1352, 156)\n"
     ]
    }
   ],
   "source": [
    "# Read train/test datasets\n",
    "dataset_train = pd.read_csv(f\"{INPUT_DIR}dataset_baseline.csv\")\n",
    "dataset_test = pd.read_csv(f\"{EVAL_DIR}dataset_baseline.csv\")\n",
    "print(dataset_train.shape, dataset_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_station_stats(df, station_col, value_col):\n",
    "    return df.groupby(station_col)[value_col].agg([\n",
    "        \"median\",\n",
    "        (\"iqr\", lambda x: iqr(x))\n",
    "    ]).to_dict(orient=\"index\")\n",
    "\n",
    "def normalize_station(df, y_column):\n",
    "    median = df[y_column].median()\n",
    "    iqr_value = iqr(df[y_column])\n",
    "    df[f\"{y_column}_normalized\"] = (df[y_column] - median) / iqr_value # Robust Scaling (IQR Scaling)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m6/rpntmn7d1j3319llfgc9lhq40000gn/T/ipykernel_4271/4015224528.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dataset_train = dataset_train.groupby(\"station_code\").apply(lambda df: normalize_station(df, column)).reset_index(drop=True)\n",
      "/var/folders/m6/rpntmn7d1j3319llfgc9lhq40000gn/T/ipykernel_4271/4015224528.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dataset_train = dataset_train.groupby(\"station_code\").apply(lambda df: normalize_station(df, column)).reset_index(drop=True)\n",
      "/var/folders/m6/rpntmn7d1j3319llfgc9lhq40000gn/T/ipykernel_4271/4015224528.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dataset_train = dataset_train.groupby(\"station_code\").apply(lambda df: normalize_station(df, column)).reset_index(drop=True)\n",
      "/var/folders/m6/rpntmn7d1j3319llfgc9lhq40000gn/T/ipykernel_4271/4015224528.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  dataset_train = dataset_train.groupby(\"station_code\").apply(lambda df: normalize_station(df, column)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28431, 164)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "station_stats = {}\n",
    "for column in COLUMNS_TO_DROP_STS:\n",
    "    station_stats[column] = compute_station_stats(dataset_train, \"station_code\", column)\n",
    "    dataset_train = dataset_train.groupby(\"station_code\").apply(lambda df: normalize_station(df, column)).reset_index(drop=True)\n",
    "# dataset_train = dataset_train.set_index([\"ObsDate\", \"station_code\"])\n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29783, 162)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_full = pd.concat([dataset_train, dataset_test])\n",
    "dataset_full = dataset_full.set_index([\"ObsDate\", \"station_code\"])\n",
    "dataset_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS_TO_DROP_BSTS_NORM = COLUMNS_TO_DROP_STS+[f\"{col}_normalized\" for col in COLUMNS_TO_DROP_STS]\n",
    "X_train = dataset_full.drop(columns=COLUMNS_TO_DROP_BSTS_NORM)\n",
    "y_train = {}\n",
    "for i in range(0, NUMBER_OF_WEEK):\n",
    "    y_train[i] = dataset_full[f\"water_flow_week{i+1}_normalized\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical station IDs to indices\n",
    "station_codes = X_train.reset_index()['station_code'].apply(lambda x: str(x)[:3]).astype(\"category\").cat.codes.values.astype(np.int32)\n",
    "num_stations = X_train.reset_index()['station_code'].apply(lambda x: str(x)[:3]).nunique()\n",
    "\n",
    "# Define station embeddings (learnable station-specific parameters)\n",
    "station_embedding_dim = 3  # Adjust as needed\n",
    "station_embeddings = tf.Variable(tf.random.normal([num_stations, station_embedding_dim]), name=\"station_embeddings\")\n",
    "\n",
    "# Station embeddings as hierarchical factors\n",
    "station_inputs = tf.gather(station_embeddings, station_codes)  # Get station-specific embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bsts_model(observed_time_series):\n",
    "  features_effect = tfp.sts.SparseLinearRegression(\n",
    "      design_matrix=np.hstack([X_train.values, station_inputs.numpy()]).astype(np.float32), name='features_effect')\n",
    "  observation_noise_prior = tfp.distributions.LogNormal(loc=0.0, scale=1.0)  # Prior with higher variance\n",
    "\n",
    "  model = tfp.sts.Sum([features_effect],\n",
    "                      observed_time_series=observed_time_series,\n",
    "                      observation_noise_scale_prior=observation_noise_prior\n",
    "                    )\n",
    "  return model\n",
    "\n",
    "\n",
    "def train_bayesian_sts(y_training_data, model):\n",
    "    # Build the variational surrogate posteriors `qs`.\n",
    "    variational_posteriors = tfp.sts.build_factored_surrogate_posterior(model=model, seed=SEED)\n",
    "    num_variational_steps = 200 \n",
    "    # Build and optimize the variational loss function.\n",
    "    elbo_loss_curve = tfp.vi.fit_surrogate_posterior(\n",
    "        target_log_prob_fn=model.joint_distribution(\n",
    "            observed_time_series=y_training_data).log_prob,\n",
    "        surrogate_posterior=variational_posteriors,\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=0.1),\n",
    "        num_steps=num_variational_steps,\n",
    "        jit_compile=True,\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    return elbo_loss_curve, variational_posteriors\n",
    "\n",
    "\n",
    "def aggregate_by_prefix(data, prefix_length):\n",
    "    grouped_data = {}\n",
    "\n",
    "    for key, stats in data.items():\n",
    "        prefix = str(key)[:prefix_length]  # Extract first 3 digits if prefix=3\n",
    "        if prefix not in grouped_data:\n",
    "            grouped_data[prefix] = {\"median\": [], \"iqr\": []}\n",
    "        grouped_data[prefix][\"median\"].append(stats[\"median\"])\n",
    "        grouped_data[prefix][\"iqr\"].append(stats[\"iqr\"])\n",
    "\n",
    "    # Calculate the mean of the medians and IQRs for each group\n",
    "    aggregated_stats = {\n",
    "        prefix: {\n",
    "            \"median\": np.mean(values[\"median\"]),\n",
    "            \"iqr\": np.mean(values[\"iqr\"])\n",
    "        }\n",
    "        for prefix, values in grouped_data.items()\n",
    "    }\n",
    "\n",
    "    return aggregated_stats\n",
    "\n",
    "\n",
    "def denormalize_predictions(df, y_pred_normalized, column_name):\n",
    "    y_pred_real = []\n",
    "    station_stats_prefix3 = aggregate_by_prefix(station_stats[column_name], 3)\n",
    "    station_stats_prefix2 = aggregate_by_prefix(station_stats[column_name], 2)\n",
    "    station_stats_prefix1 = aggregate_by_prefix(station_stats[column_name], 1)\n",
    "    for station, y_pred in zip(df[\"station_code\"], y_pred_normalized):\n",
    "        if station in station_stats[column_name].keys():\n",
    "            median, iqr_value = station_stats[column_name][station].values()  # Retrieve station stats\n",
    "\n",
    "        elif str(station)[:3] in station_stats_prefix3.keys():\n",
    "            median, iqr_value = station_stats_prefix3[str(station)[:3]].values()\n",
    "\n",
    "        elif str(station)[:2] in station_stats_prefix2.keys():\n",
    "            median, iqr_value = station_stats_prefix2[str(station)[:2]].values()\n",
    "\n",
    "        elif str(station)[:1] in station_stats_prefix1.keys():\n",
    "            median, iqr_value = station_stats_prefix1[str(station)[:1]].values()\n",
    "\n",
    "        reverse_norm = (y_pred * iqr_value) + median  # Denormalization\n",
    "        y_pred_real.append(reverse_norm)  \n",
    "    \n",
    "    return y_pred_real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training week 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741190407.338305 1302870 service.cc:148] XLA service 0x11a3bf320 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1741190407.338686 1302870 service.cc:156]   StreamExecutor device (0): Host, Default Version\n",
      "2025-03-05 17:00:07.492662: W tensorflow/compiler/tf2xla/kernels/random_ops.cc:107] Warning: Using tf.random.uniform with XLA compilation will ignore seeds; consider using tf.random.stateless_uniform instead if reproducible behavior is desired. fit_surrogate_posterior/sanitize_seed/seed\n",
      "2025-03-05 17:00:07.522152: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1741190411.593498 1302870 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training week 1\n",
      "Training week 2\n",
      "Training week 3\n"
     ]
    }
   ],
   "source": [
    "### ----- Training section ----- ###\n",
    "\n",
    "num_forecast_steps = 1352 # 6 years.\n",
    "n_sample = 500\n",
    "for index, y_train_week in y_train.items():\n",
    "    print(f\"Training week {index}\")\n",
    "    y_training_data = y_train_week[:-num_forecast_steps].values.astype(np.float32)\n",
    "    model = build_bsts_model(y_training_data)\n",
    "    _, variational_posteriors = train_bayesian_sts(y_training_data, model)\n",
    "    # Draw samples from the variational posterior.\n",
    "    q_samples = variational_posteriors.sample(n_sample)\n",
    "    # Sauvegarder avec TensorFlow SavedModel\n",
    "    # time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    posterior_path = f\"{MODEL_DIR}final/bsts_vi_posterior_samples_week_{index}.npy\"\n",
    "    np.save(posterior_path, q_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- Forecast section ----- ###\n",
    "\n",
    "forecast_df = pd.DataFrame({\n",
    "    \"ObsDate\": dataset_test['ObsDate'].values,\n",
    "    \"station_code\": dataset_test['station_code'].values,\n",
    "})\n",
    "for index, y_train_week in y_train.items():\n",
    "    y_training_data = y_train_week[:-num_forecast_steps].values.astype(np.float32)\n",
    "    model = build_bsts_model(y_training_data)\n",
    "    # Load the saved posterior samples:\n",
    "    loaded_posterior_samples = np.load(f\"{MODEL_DIR}final/bsts_vi_posterior_samples_week_{index}.npy\", allow_pickle=True).item()\n",
    "\n",
    "    # Generate forecast samples\n",
    "    forecast_dist = tfp.sts.forecast(\n",
    "        model=model,\n",
    "        observed_time_series=y_training_data,\n",
    "        parameter_samples=loaded_posterior_samples,\n",
    "        num_steps_forecast=num_forecast_steps)\n",
    "    mean_forecast = forecast_dist.mean().numpy()\n",
    "    forecast_samples = forecast_dist.sample(n_sample).numpy()\n",
    "    # Compute 70% credible interval (percentiles 15% and 85%) for each forecast\n",
    "    forecast_lower = np.percentile(forecast_samples, 30, axis=0)\n",
    "    forecast_upper = np.percentile(forecast_samples, 70, axis=0)\n",
    "    # Denormalisation\n",
    "    mean_forecast_denorm = denormalize_predictions(forecast_df, mean_forecast.flatten(), f\"water_flow_week{index+1}\")\n",
    "    forecast_lower_denorm = denormalize_predictions(forecast_df, forecast_lower.flatten(), f\"water_flow_week{index+1}\")\n",
    "    forecast_upper_denorm = denormalize_predictions(forecast_df, forecast_upper.flatten(), f\"water_flow_week{index+1}\")\n",
    "    forecast_df[f\"week_{index}_pred\"] = mean_forecast_denorm\n",
    "    forecast_df[f\"week_{index}_inf\"] = forecast_lower_denorm\n",
    "    forecast_df[f\"week_{index}_sup\"] = forecast_upper_denorm\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save forecast series\n",
    "forecast_df.to_csv(f\"{EVAL_DIR}/predictions_bsts.csv\", index=False)\n",
    "# Create a ZIP file containing predictions.csv\n",
    "with zipfile.ZipFile(f\"{EVAL_DIR}predictions_bsts.zip\", 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    zipf.write(f\"{EVAL_DIR}predictions_bsts.csv\", \"predictions_bsts.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Performance Evaluation on the Full Training Set\n",
    "\n",
    "> **Note:**  \n",
    "> The performance displayed here is calculated on the training set. This does not necessarily reflect the models' performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_stations = dataset_train[\"station_code\"].values\n",
    "\n",
    "for i in range(NUMBER_OF_WEEK):\n",
    "    predictions = []\n",
    "    baseline_day_before = dataset_train[\"water_flow_lag_1w\"]\n",
    "    predictions.append({\"model\": \"Week before\", \"prediction\": baseline_day_before, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "    if \"mapie\" in FINAL_MODELS:\n",
    "        y_pred_mapie, y_pis_mapie = mapie[i].predict(X_train)\n",
    "        predictions.append({\"model\": \"LGBM+MAPIE\", \"prediction\": y_pred_mapie, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_mapie})\n",
    "    if \"qrf\" in FINAL_MODELS:\n",
    "        y_pred_qrf = qrf[i].predict(X_train, quantiles=\"mean\", aggregate_leaves_first=False)\n",
    "        y_pis_qrf = qrf[i].predict(X_train, quantiles=[ALPHA/2, 1-ALPHA/2])\n",
    "        predictions.append({\"model\": \"QRF\", \"prediction\": y_pred_qrf, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_qrf})\n",
    "    if \"ebm\" in FINAL_MODELS:\n",
    "        y_pred_ebm = []\n",
    "        for model in ebm_ensembles[i]:\n",
    "            y_pred_ebm.append(model.predict(X_train))\n",
    "        y_pred_ebm = np.mean(y_pred_ebm, axis=0)\n",
    "        predictions.append({\"model\": \"EBM\", \"prediction\": y_pred_ebm, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "\n",
    "    compare_models_per_station(\n",
    "        y_train[i].values,\n",
    "        predictions,\n",
    "        y_train_stations,\n",
    "        column_to_display=\"log_likelihood\" ,\n",
    "        title = f\"WEEK {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Coverage on the Full Training Set\n",
    "\n",
    "> **Note:**  \n",
    "> The performance displayed here is calculated on the training set. This does not necessarily reflect the models' performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUMBER_OF_WEEK):\n",
    "\n",
    "    predictions = []\n",
    "    baseline_day_before = dataset_train[\"water_flow_lag_1w\"]\n",
    "    predictions.append({\"model\": \"Week before\", \"prediction\": baseline_day_before, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "    if \"mapie\" in FINAL_MODELS:\n",
    "        y_pred_mapie, y_pis_mapie = mapie[i].predict(X_train)\n",
    "        predictions.append({\"model\": \"LGBM+MAPIE\", \"prediction\": y_pred_mapie, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_mapie})\n",
    "        coverage = (y_train[i].values >= y_pis_mapie[:,0,0]) & (y_train[i].values <= y_pis_mapie[:,1,0])\n",
    "        print(f\"MAPIE coverage of the prediction interval for week {i}: {coverage.mean()}\")\n",
    "    if \"qrf\" in FINAL_MODELS:\n",
    "        y_pred_qrf = qrf[i].predict(X_train, quantiles=\"mean\", aggregate_leaves_first=False)\n",
    "        y_pis_qrf = qrf[i].predict(X_train, quantiles=[ALPHA/2, 1-ALPHA/2])\n",
    "        predictions.append({\"model\": \"QRF\", \"prediction\": y_pred_qrf, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_qrf})\n",
    "        coverage = (y_train[i].values >= y_pis_qrf[:,0]) & (y_train[i].values <= y_pis_qrf[:,1])\n",
    "        print(f\"QRF coverage of the prediction interval for week {i}: {coverage.mean()}\")\n",
    "    if \"ebm\" in FINAL_MODELS:\n",
    "        y_pred_ebm = []\n",
    "        for model in ebm_ensembles[i]:\n",
    "            y_pred_ebm.append(model.predict(X_train))\n",
    "        y_pred_ebm = np.mean(y_pred_ebm, axis=0)\n",
    "        predictions.append({\"model\": \"EBM\", \"prediction\": y_pred_ebm, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "        coverage = (y_train[i].values >= y_pis_qrf[:,0]) & (y_train[i].values <= y_pis_qrf[:,1])\n",
    "        print(f\"EBM coverage of the prediction interval for week {i}: {coverage.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
