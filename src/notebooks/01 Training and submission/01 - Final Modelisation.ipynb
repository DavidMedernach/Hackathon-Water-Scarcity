{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Training the *Final* Models\n",
    "\n",
    "This notebook trains the model on the full *baseline_dataset* for the final prediction on evaluation data.\n",
    "\n",
    "Here, we train a model designed to generalize across water stations in Brazil and France. However, you are not required to follow this approach and may opt to train separate models for different geographic *regions*.\n",
    "\n",
    "This baseline model training example utilizes all available features, with hyperparameters chosen for quick execution rather than optimization. For hyperparameter tuning and feature selection explorations, refer to the `02_exploration` folder.\n",
    "\n",
    "> **Note:** This notebook requires outputs from the `00 Preprocessing` notebooks.\n",
    "\n",
    "<img src=\"../images/notebook-3.png\" alt=\"Experiment Diagram\" style=\"width:75%; text-align:center;\" />\n",
    "\n",
    "### 1. Data Import and Setup\n",
    "\n",
    "This section imports the necessary libraries, sets up environment paths, and includes custom utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "from mapie.regression import MapieQuantileRegressor\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..')))\n",
    "\n",
    "from src.utils.model import split_dataset, compare_models_per_station"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constants :\n",
    "- **INPUT_DIR**: Directory for input data (same as in \"02 - Feature Engineering\").\n",
    "- **MODEL_DIR**: Directory where trained models are saved.\n",
    "- **DATASET_DIR**: Directory where the Zenodo dataset is unzipped.\n",
    "\n",
    "##### Model Parameters\n",
    "\n",
    "- **SEED**: 42 (for reproducibility)\n",
    "- **NUMBER_OF_WEEK**: 4 (one model is trained per week)\n",
    "\n",
    "##### FINAL_MODELS\n",
    "\n",
    "- **mapie**: Combines LightGBM with MAPIE. **MAPIE** (Model Agnostic Prediction Interval Estimator) computes prediction intervals for any regression model using conformal methods.\n",
    "- **qrf**: Quantile Random Forest (natively produces prediction intervals)\n",
    "- **ebm**: Explainable Boosting Machine is used as a exemple that does not natively implement prediction intervals, but that can be customised to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../../../data/input/\"\n",
    "MODEL_DIR = \"../../../models/\"\n",
    "DATASET_DIR = \"../../../dataset/\"\n",
    "\n",
    "SEED = 42\n",
    "NUMBER_OF_WEEK = 4 # Number of weeks to predict one model is trained per week\n",
    "\n",
    "FINAL_MODELS = [\"mapie\",\n",
    "                \"qrf\",\n",
    "                #\"EBM\"\n",
    "                ]\n",
    "mapie_enbpi = {}\n",
    "mapie = {}\n",
    "qrf = {}\n",
    "mapie_aci = {}\n",
    "\n",
    "COLUMNS_TO_DROP = [\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading\n",
    "Load in the baseline datasets, create the directory to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv(f\"{INPUT_DIR}dataset_baseline.csv\")\n",
    "\n",
    "dataset_train = dataset_train.set_index(\"ObsDate\")\n",
    "\n",
    "if not os.path.exists(f\"{MODEL_DIR}final/\"):\n",
    "    os.makedirs(f\"{MODEL_DIR}final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing removal of unnecessary columns, setup of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset_train.drop(columns=COLUMNS_TO_DROP)\n",
    "y_train = {}\n",
    "for i in range(0, NUMBER_OF_WEEK):\n",
    "    y_train[i] = dataset_train[f\"water_flow_week{i+1}\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Models training\n",
    "#### a. LGBM + MAPIE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapie Model Training Overview\n",
    "\n",
    "- **Configuration:**  \n",
    "  - Sets `ALPHA` (0.1) as the prediction interval level.\n",
    "  - Defines `TIME_VALIDATION` as a split point for creating a validation set.\n",
    "  - Configures LightGBM parameters (`LGBM_PARAMS`) for quantile regression.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.1\n",
    "TIME_VALIDATION = \"2000-01-01 00:00:00\"\n",
    "\n",
    "LGBM_PARAMS = {\n",
    "    \"max_depth\": 15,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"n_estimators\": 500,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"objective\": \"quantile\",\n",
    "    \"alpha\": ALPHA\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Data Preparation:**  \n",
    "  - Splits `dataset_train` into training and validation subsets using `split_dataset`.\n",
    "  - Removes unnecessary columns from both the training and validation datasets.\n",
    "  - Extracts target variables for each week (from `water_flow_week1` to `water_flow_week4`).\n",
    "\n",
    "- **Model Training:**  \n",
    "  For each week:\n",
    "  - Initializes a LightGBM regressor with the specified parameters.\n",
    "  - Wraps it in a `MapieQuantileRegressor` to estimate prediction intervals.\n",
    "  - Trains the model on the training data and calibrates it using the validation data.\n",
    "  - Saves the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Mapie\n",
      "154\n",
      "Training week 0\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 0.218643\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008569 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 541.075745\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002580 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 10.800000\n",
      "Training week 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001701 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 0.218929\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002068 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 542.302002\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 10.837143\n",
      "Training week 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001956 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 0.219071\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002311 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 543.414124\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001757 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 10.885586\n",
      "Training week 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001167 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 0.219143\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001676 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 544.039490\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002219 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 11458\n",
      "[LightGBM] [Info] Number of data points in the train set: 15051, number of used features: 154\n",
      "[LightGBM] [Info] Start training from score 10.898714\n"
     ]
    }
   ],
   "source": [
    "if \"mapie\" in FINAL_MODELS: \n",
    "    print(\"Training Mapie\")\n",
    "\n",
    "\n",
    "    train_mapie, val_mapie, val_temporal  = split_dataset(dataset_train, 0.75, TIME_VALIDATION)\n",
    "\n",
    "    X_train_mapie = train_mapie.drop(columns=COLUMNS_TO_DROP)\n",
    "    print(len(X_train_mapie.columns))\n",
    "    y_train_mapie = {}\n",
    "    for i in range(0, NUMBER_OF_WEEK):\n",
    "        y_train_mapie[i] = train_mapie[f\"water_flow_week{i+1}\"]\n",
    "\n",
    "    X_val = val_mapie.drop(columns=COLUMNS_TO_DROP)\n",
    "    y_val = {}\n",
    "    y_val[0] = val_mapie[\"water_flow_week1\"]\n",
    "    for i in range(1, NUMBER_OF_WEEK):\n",
    "        y_val[i] = val_mapie[f\"water_flow_week{i+1}\"]\n",
    "\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training week {i}\")\n",
    "        # Initialize and train MapieQuantileRegressor\n",
    "        regressor = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "        mapie[i] = MapieQuantileRegressor(estimator=regressor, method=\"quantile\", cv=\"split\", alpha=ALPHA)\n",
    "        mapie[i].fit(X_train_mapie, y_train_mapie[i], X_calib=X_val, y_calib=y_val[i])\n",
    "        \n",
    "        # save model with date\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        model_path = f\"{MODEL_DIR}final/mapie_quantile_{time}_week_{i}.pkl\"\n",
    "        joblib.dump(mapie[i], model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. QRF\n",
    "\n",
    "- **Training:**  \n",
    "  Initializes a `RandomForestQuantileRegressor` with the following parameters:\n",
    "  - 100 estimators\n",
    "  - Maximum depth of 10\n",
    "  - Minimum of 10 samples per leaf\n",
    "\n",
    "  These parameters allow for relatively fast training, though they are not optimized for peak performance. \n",
    "  \n",
    "  The model is then fitted using `X_train` and the corresponding weekly target `y_train[i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training week 0\n",
      "Training week 1\n",
      "Training week 2\n",
      "Training week 3\n"
     ]
    }
   ],
   "source": [
    "if \"qrf\" in FINAL_MODELS:\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training week {i}\")\n",
    "        # Train RandomForestQuantileRegressor\n",
    "        qrf[i] = RandomForestQuantileRegressor(n_estimators=100, max_depth=5, min_samples_leaf=5)\n",
    "        qrf[i].fit(X_train, y_train[i])\n",
    "\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        model_path = f\"{MODEL_DIR}final/qrf_quantile_{time}_week_{i}.pkl\"\n",
    "        joblib.dump(qrf[i], model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Explainable Boosting Machine\n",
    "\n",
    "EBM is an ensemble method that does not natively provide access to its individual members for performing quantile predictions or generating prediction intervals. To overcome this limitation, we manually construct an ensemble.\n",
    "\n",
    "\n",
    "- **Ensemble Training:**  \n",
    "- For each ensemble member (seed from 0 to 4):\n",
    "    - A bootstrap sample is created from `X_train` and `y_train[i]` using sampling with replacement.\n",
    "    - An `ExplainableBoostingRegressor` is instantiated with fixed parameters (e.g., `max_bins=128`, `learning_rate=0.05`, `interactions=3`, and `random_state=42` to ensure consistent binning) and then trained on the sampled data.\n",
    "    - The trained model is appended to the list for the current week.\n",
    "- **Saving the Ensemble:**  \n",
    "- The ensemble (i.e., the list of EBM models for the week) is saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ebm\" in FINAL_MODELS:\n",
    "    NUM_ENSEMBLES = 5\n",
    "    ebm_ensembles = {}\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training EBM ensemble for week {i}\")\n",
    "\n",
    "        models_i = []\n",
    "        for seed in range(NUM_ENSEMBLES):\n",
    "            print(f\"Training EBM ensemble {seed} for week {i}\")\n",
    "            # 1. Create your bootstrap sample or subset (if you want bagging)\n",
    "            sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "            X_sample = X_train.iloc[sample_indices]\n",
    "            y_sample = y_train[i][sample_indices]\n",
    "            \n",
    "            # 2. Train an EBM with consistent binning parameters\n",
    "            ebm_model = ExplainableBoostingRegressor(\n",
    "                outer_bags=1,\n",
    "                inner_bags=1,\n",
    "                max_bins=128,\n",
    "                learning_rate=0.05,\n",
    "                interactions=3,\n",
    "                early_stopping_rounds=100,\n",
    "                random_state=SEED\n",
    "            )\n",
    "            ebm_model.fit(X_sample, y_sample)\n",
    "            \n",
    "            models_i.append(ebm_model)\n",
    "\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        file_path = f\"{MODEL_DIR}final/ebm_ensemble_{time}_week_{i}.pkl\"\n",
    "\n",
    "        joblib.dump(ebm_ensembles, file_path)\n",
    "        print(f\"Saved EBM ensembles to {file_path}\")\n",
    "\n",
    "        # Store the list of models for week i\n",
    "        ebm_ensembles[i] = models_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Performance Evaluation on the Full Training Set\n",
    "\n",
    "> **Note:**  \n",
    "> The performance displayed here is calculated on the training set. This does not necessarily reflect the models' performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davidmedernach/python_projects/Hackathon_2025/public/Hackathon-Water-Scarcity/.venv/lib/python3.12/site-packages/mapie/utils.py:620: UserWarning: WARNING: The predictions are ill-sorted.\n",
      "  warnings.warn(\n",
      "/Users/davidmedernach/python_projects/Hackathon_2025/public/Hackathon-Water-Scarcity/.venv/lib/python3.12/site-packages/mapie/utils.py:620: UserWarning: WARNING: The predictions are ill-sorted.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_per_station_metrics() got an unexpected keyword argument 'y_pred_intervals'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     y_pred_ebm \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(y_pred_ebm, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEBM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_pred_ebm, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstations\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_train_stations, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction_interval\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m})\n\u001b[0;32m---> 21\u001b[0m \u001b[43mcompare_models_per_station\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train_stations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn_to_display\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlog_likelihood\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWEEK \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/python_projects/Hackathon_2025/public/Hackathon-Water-Scarcity/src/utils/model.py:384\u001b[0m, in \u001b[0;36mcompare_models_per_station\u001b[0;34m(y, predictions, station_code, prefix, column_to_display, title, save, display)\u001b[0m\n\u001b[1;32m    378\u001b[0m y_pred_std \u001b[38;5;241m=\u001b[39m standardize_values(y_pred, station_code, station_stats)\n\u001b[1;32m    380\u001b[0m y_pred_lower_std, y_pred_upper_std \u001b[38;5;241m=\u001b[39m standardize_prediction_intervals(\n\u001b[1;32m    381\u001b[0m     y_pred_intervals, station_code, station_stats\n\u001b[1;32m    382\u001b[0m )\n\u001b[0;32m--> 384\u001b[0m station_metrics_df \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_per_station_metrics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_true_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstation_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred_intervals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred_intervals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred_lower_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred_lower_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred_upper_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_pred_upper_std\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    393\u001b[0m station_metrics_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model_name\n\u001b[1;32m    394\u001b[0m all_station_metrics\u001b[38;5;241m.\u001b[39mappend(station_metrics_df)\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_per_station_metrics() got an unexpected keyword argument 'y_pred_intervals'"
     ]
    }
   ],
   "source": [
    "y_train_stations = dataset_train[\"station_code\"].values\n",
    "\n",
    "for i in range(NUMBER_OF_WEEK):\n",
    "    predictions = []\n",
    "    baseline_day_before = dataset_train[\"water_flow_lag_1w\"]\n",
    "    predictions.append({\"model\": \"Week before\", \"prediction\": baseline_day_before, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "    if \"mapie\" in FINAL_MODELS:\n",
    "        y_pred_mapie, y_pis_mapie = mapie[i].predict(X_train)\n",
    "        predictions.append({\"model\": \"LGBM+MAPIE\", \"prediction\": y_pred_mapie, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_mapie})\n",
    "    if \"qrf\" in FINAL_MODELS:\n",
    "        y_pred_qrf = qrf[i].predict(X_train, quantiles=\"mean\", aggregate_leaves_first=False)\n",
    "        y_pis_qrf = qrf[i].predict(X_train, quantiles=[ALPHA/2, 1-ALPHA/2])\n",
    "        predictions.append({\"model\": \"QRF\", \"prediction\": y_pred_qrf, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_qrf})\n",
    "    if \"ebm\" in FINAL_MODELS:\n",
    "        y_pred_ebm = []\n",
    "        for model in ebm_ensembles[i]:\n",
    "            y_pred_ebm.append(model.predict(X_train))\n",
    "        y_pred_ebm = np.mean(y_pred_ebm, axis=0)\n",
    "        predictions.append({\"model\": \"EBM\", \"prediction\": y_pred_ebm, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "\n",
    "    compare_models_per_station(\n",
    "        y_train[i].values,\n",
    "        predictions,\n",
    "        y_train_stations,\n",
    "        column_to_display=\"log_likelihood\" ,\n",
    "        title = f\"WEEK {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Coverage on the Full Training Set\n",
    "\n",
    "> **Note:**  \n",
    "> The performance displayed here is calculated on the training set. This does not necessarily reflect the models' performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(NUMBER_OF_WEEK):\n",
    "\n",
    "    predictions = []\n",
    "    baseline_day_before = dataset_train[\"water_flow_lag_1w\"]\n",
    "    predictions.append({\"model\": \"Week before\", \"prediction\": baseline_day_before, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "    if \"mapie\" in FINAL_MODELS:\n",
    "        y_pred_mapie, y_pis_mapie = mapie[i].predict(X_train)\n",
    "        predictions.append({\"model\": \"LGBM+MAPIE\", \"prediction\": y_pred_mapie, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_mapie})\n",
    "        coverage = (y_train[i].values >= y_pis_mapie[:,0,0]) & (y_train[i].values <= y_pis_mapie[:,1,0])\n",
    "        print(f\"MAPIE coverage of the prediction interval for week {i}: {coverage.mean()}\")\n",
    "    if \"qrf\" in FINAL_MODELS:\n",
    "        y_pred_qrf = qrf[i].predict(X_train, quantiles=\"mean\", aggregate_leaves_first=False)\n",
    "        y_pis_qrf = qrf[i].predict(X_train, quantiles=[ALPHA/2, 1-ALPHA/2])\n",
    "        predictions.append({\"model\": \"QRF\", \"prediction\": y_pred_qrf, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_qrf})\n",
    "        coverage = (y_train[i].values >= y_pis_qrf[:,0]) & (y_train[i].values <= y_pis_qrf[:,1])\n",
    "        print(f\"QRF coverage of the prediction interval for week {i}: {coverage.mean()}\")\n",
    "    if \"ebm\" in FINAL_MODELS:\n",
    "        y_pred_ebm = []\n",
    "        for model in ebm_ensembles[i]:\n",
    "            y_pred_ebm.append(model.predict(X_train))\n",
    "        y_pred_ebm = np.mean(y_pred_ebm, axis=0)\n",
    "        predictions.append({\"model\": \"EBM\", \"prediction\": y_pred_ebm, \"dataset\":\"train\", \"stations\": y_train_stations, \"prediction_interval\": None})\n",
    "        coverage = (y_train[i].values >= y_pis_qrf[:,0]) & (y_train[i].values <= y_pis_qrf[:,1])\n",
    "        print(f\"EBM coverage of the prediction interval for week {i}: {coverage.mean()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
