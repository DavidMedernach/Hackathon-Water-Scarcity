{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals: Training of the *Final* model\n",
    "\n",
    "This notebook train on the full *baseline_dataset* the model used for final prediction on evaluation data.\n",
    "We are training here a single model that aims to generalize on Brazil and France Water stations, you don't have to do the same and can potentially train different models for different geographic *areas*.\n",
    "\n",
    "\n",
    "> Note this notebook need ouputs from *01 Preprocessing and Training/02 - Feature Engineering*\n",
    "\n",
    "\n",
    "# 1. Data Import and Setup\n",
    "\n",
    "Imports necessary libraries, sets up environment paths, and includes custom utility functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "import copy\n",
    "import joblib\n",
    "import importlib\n",
    "\n",
    "from mapie.regression import MapieQuantileRegressor\n",
    "from mapie.regression import MapieTimeSeriesRegressor\n",
    "\n",
    "# Import the EBMRegression estimator from Synapse ML.\n",
    "from interpret.glassbox import ExplainableBoostingRegressor, LinearRegression, RegressionTree\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..','..','..')))\n",
    "\n",
    "from src.utils.model import split_dataset, compare_models_per_station, load_models_auto\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defines constants :\n",
    "* INPUT_DIR must be the same as the one defined in *02 - Feature Engineering*.\n",
    "* Models will be saved in *MODEL_DIR*\n",
    "* *DATASET_DIR* must be the directory where you unzip zenodo dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../../../data/input/\"\n",
    "MODEL_DIR = \"../../../models/\"\n",
    "DATASET_DIR = \"../../../dataset/\"\n",
    "\n",
    "SEED = 42\n",
    "NUMBER_OF_WEEK = 4 # Number of weeks to predict one model is trained per week\n",
    "\n",
    "FINAL_MODELS = [\"mapie\",\n",
    "                \"qrf\",\n",
    "                #\"EBM\",\n",
    "                #\"aci\"\n",
    "                ]\n",
    "mapie_enbpi = {}\n",
    "mapie = {}\n",
    "qrf = {}\n",
    "mapie_aci = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading\n",
    "Load in the baseline datasets, create the directory to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "dataset_train = pd.read_csv(f\"{INPUT_DIR}dataset_baseline.csv\")\n",
    "\n",
    "dataset_train = dataset_train.set_index(\"ObsDate\")\n",
    "\n",
    "# create path to save models if it does not exist\n",
    "\n",
    "if not os.path.exists(f\"{MODEL_DIR}final/\"):\n",
    "    os.makedirs(f\"{MODEL_DIR}final/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing removal of unnecessary columns, setup of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset_train.drop(columns=[\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\", \"index\"])\n",
    "y_train = {}\n",
    "for i in range(0, NUMBER_OF_WEEK):\n",
    "    y_train[i] = dataset_train[f\"water_flow_week{i+1}\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models training\n",
    "### a. LGBM + MAPIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mapie\" in FINAL_MODELS: \n",
    "    print(\"Training Mapie\")\n",
    "    # Define constants\n",
    "    ALPHA = 0.1\n",
    "    TIME_VALIDATION = \"2000-01-01 00:00:00\"\n",
    "    LGBM_PARAMS = {\n",
    "        \"max_depth\": 15,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"n_estimators\": 500,\n",
    "        \"colsample_bytree\": 0.7,\n",
    "        \"objective\": \"quantile\",\n",
    "        \"alpha\": ALPHA\n",
    "    }\n",
    "\n",
    "    train_mapie, val_mapie, val_temporal  = split_dataset(dataset_train, 0.75, TIME_VALIDATION)\n",
    "\n",
    "    X_train_mapie = train_mapie.drop(columns=[\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\", \"index\"])\n",
    "    print(len(X_train_mapie.columns))\n",
    "    y_train_mapie = {}\n",
    "    for i in range(0, NUMBER_OF_WEEK):\n",
    "        y_train_mapie[i] = train_mapie[f\"water_flow_week{i+1}\"]\n",
    "\n",
    "    X_val = val_mapie.drop(columns=[\"water_flow_week1\", \"station_code\", \"water_flow_week2\", \"water_flow_week3\", \"water_flow_week4\", \"index\"])\n",
    "    y_val = {}\n",
    "    y_val[0] = val_mapie[\"water_flow_week1\"]\n",
    "    for i in range(1, NUMBER_OF_WEEK):\n",
    "        y_val[i] = val_mapie[f\"water_flow_week{i+1}\"]\n",
    "\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training week {i}\")\n",
    "        # Initialize and train MapieQuantileRegressor\n",
    "        regressor = lgb.LGBMRegressor(**LGBM_PARAMS)\n",
    "        mapie[i] = MapieQuantileRegressor(estimator=regressor, method=\"quantile\", cv=\"split\", alpha=ALPHA)\n",
    "        mapie[i].fit(X_train_mapie, y_train_mapie[i], X_calib=X_val, y_calib=y_val[i])\n",
    "        \n",
    "        # save model with date\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "        model_path = f\"{MODEL_DIR}final/mapie_quantile_{time}_week_{i}.pkl\"\n",
    "        joblib.dump(mapie[i], model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. QRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"qrf\" in FINAL_MODELS:\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training week {i}\")\n",
    "        # Train RandomForestQuantileRegressor\n",
    "        qrf[i] = RandomForestQuantileRegressor(n_estimators=100, max_depth=10, min_samples_leaf=10)\n",
    "        qrf[i].fit(X_train, y_train[i])\n",
    "\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        model_path = f\"{MODEL_DIR}final/qrf_quantile_{time}_week_{i}.pkl\"\n",
    "        joblib.dump(qrf[i], model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Explainable Boosting Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ebm\" in FINAL_MODELS:\n",
    "    NUM_ENSEMBLES = 5\n",
    "\n",
    "    # A dictionary to hold the list of EBM models per week\n",
    "    ebm_ensembles = {}\n",
    "\n",
    "    for i in range(NUMBER_OF_WEEK):\n",
    "        print(f\"Training EBM ensemble for week {i}\")\n",
    "\n",
    "        # This will store all seed models for a single week\n",
    "        models_i = []\n",
    "        \n",
    "        for seed in range(NUM_ENSEMBLES):\n",
    "            print(f\"Training EBM ensemble {seed} for week {i}\")\n",
    "            # 1. Create your bootstrap sample or subset (if you want bagging)\n",
    "            sample_indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "            X_sample = X_train.iloc[sample_indices]\n",
    "            y_sample = y_train[i][sample_indices]\n",
    "            \n",
    "            # 2. Train an EBM with consistent binning parameters\n",
    "            ebm_model = ExplainableBoostingRegressor(\n",
    "                outer_bags=1,\n",
    "                inner_bags=1,\n",
    "                max_bins=128,\n",
    "                learning_rate=0.05,\n",
    "                interactions=3,\n",
    "                early_stopping_rounds=100,\n",
    "                random_state=42  # ensures same binning\n",
    "            )\n",
    "            ebm_model.fit(X_sample, y_sample)\n",
    "            \n",
    "            models_i.append(ebm_model)\n",
    "\n",
    "        time = pd.Timestamp.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        file_path = f\"{MODEL_DIR}final/ebm_ensemble_{time}_week_{i}.pkl\"\n",
    "\n",
    "        joblib.dump(ebm_ensembles, file_path)\n",
    "        print(f\"Saved EBM ensembles to {file_path}\")\n",
    "\n",
    "        # Store the list of models for week i\n",
    "        ebm_ensembles[i] = models_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_stations = dataset_train[\"station_code\"].values\n",
    "\n",
    "for i in range(NUMBER_OF_WEEK):\n",
    "\n",
    "    print(f\"============================== WEEK {i} temporal ===================================\")\n",
    "    baseline_day_before = dataset_train[\"water_flow_lag_1w\"]\n",
    "    y_pred_mapie, y_pis_mapie = mapie[i].predict(X_train)\n",
    "    y_pred_qrf = qrf[i].predict(X_train, quantiles=\"mean\", aggregate_leaves_first=False)\n",
    "    y_pis_qrf = qrf[i].predict(X_train, quantiles=[ALPHA/2, 1-ALPHA/2])\n",
    "\n",
    "    predictions = [\n",
    "        {\"model\": \"LGBM+MAPIE\", \"prediction\": y_pred_mapie, \"dataset\":\"test\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_mapie},\n",
    "        {\"model\": \"Week before\", \"prediction\": baseline_day_before, \"dataset\":\"test\", \"stations\": y_train_stations, \"prediction_interval\": None},\n",
    "        {\"model\": \"QRF\", \"prediction\": y_pred_qrf, \"dataset\":\"test\", \"stations\": y_train_stations, \"prediction_interval\": y_pis_qrf},\n",
    "    ]\n",
    "\n",
    "    nop = compare_models_per_station(y_train[i].values,\n",
    "                                     predictions,\n",
    "                                     y_train_stations,\n",
    "                                     column_to_display=\"log_likelihood\" ,\n",
    "                                     title = f\"WEEK {i}\")\n",
    "\n",
    "    coverage_mapie = (y_train[i].values >= y_pis_mapie[:,0,0]) & (y_train[i].values <= y_pis_mapie[:,1,0])\n",
    "    print(f\"Coverage of the prediction interval for week {i}: {coverage_mapie.mean()}\")\n",
    "\n",
    "    coverage_qrf = (y_train[i].values >= y_pis_qrf[:,0]) & (y_train[i].values <= y_pis_qrf[:,1])\n",
    "    print(f\"Coverage of the prediction interval for week {i}: {coverage_qrf.mean()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
